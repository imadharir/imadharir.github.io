<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Imad Harir</title>
        <link>https://imadharir.github.io/post/</link>
        <description>Recent content in Posts on Imad Harir</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 30 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://imadharir.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Amazon Books Scraper</title>
        <link>https://imadharir.github.io/p/amazon-books-scraper/</link>
        <pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>https://imadharir.github.io/p/amazon-books-scraper/</guid>
        <description>&lt;img src="https://imadharir.github.io/p/amazon-books-scraper/AmazonBookScraper.png" alt="Featured image of post Amazon Books Scraper" /&gt;&lt;h1 id=&#34;amazon-book-scraper-and-storage&#34;&gt;Amazon Book Scraper and Storage
&lt;/h1&gt;&lt;h2 id=&#34;project-overview&#34;&gt;Project Overview
&lt;/h2&gt;&lt;p&gt;The Amazon Book Scraper and Storage project is an automated data pipeline that scrapes book information from Amazon and stores it in a PostgreSQL database. The project leverages Apache Airflow for orchestration and scheduling of the scraping and data ingestion process.&lt;/p&gt;
&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents
&lt;/h2&gt;&lt;ol&gt;
  &lt;li&gt;Project Overview&lt;/li&gt;
  &lt;li&gt;Architecture&lt;/li&gt;
  &lt;li&gt;Features&lt;/li&gt;
  &lt;li&gt;Technologies Used&lt;/li&gt;
  &lt;li&gt;Setup and Installation&lt;/li&gt;
  &lt;li&gt;Usage&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture
&lt;/h2&gt;&lt;p&gt;
The project follows a simple ETL (Extract, Transform, Load) architecture:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Extract:&lt;/strong&gt; Data is scraped from Amazon using a Python script with &lt;code&gt;requests&lt;/code&gt; and &lt;code&gt;BeautifulSoup&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transform:&lt;/strong&gt; The extracted data is cleaned and structured using &lt;code&gt;pandas&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load:&lt;/strong&gt; The structured data is inserted into a PostgreSQL database using &lt;code&gt;airflow.providers.postgres.hooks.postgres&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://imadharir.github.io/p/amazon-books-scraper/AmazonBookScraper.png&#34;
	width=&#34;1141&#34;
	height=&#34;922&#34;
	srcset=&#34;https://imadharir.github.io/p/amazon-books-scraper/AmazonBookScraper_hu797588405123154000.png 480w, https://imadharir.github.io/p/amazon-books-scraper/AmazonBookScraper_hu9161034162332869920.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;123&#34;
		data-flex-basis=&#34;297px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;high-level-architecture-components&#34;&gt;High-Level Architecture Components:
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Airflow DAG:&lt;/strong&gt; Manages and schedules the daily data pipeline tasks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python Scraper:&lt;/strong&gt; Extracts book data (title, author, price, rating) from Amazon.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PostgreSQL Database:&lt;/strong&gt; Stores the scraped book data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google Cloud Composer:&lt;/strong&gt; Manages the Airflow orchestration in a cloud environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;features&#34;&gt;Features
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Automated Daily Scraping:&lt;/strong&gt; The pipeline runs daily, automatically scraping new book data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Deduplication:&lt;/strong&gt; Ensures only new books are added during each subsequent run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud-Based Orchestration:&lt;/strong&gt; Uses Google Cloud Composer for cloud-based execution and management.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modular Design:&lt;/strong&gt; Components are decoupled, making it easy to extend or modify parts of the pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;technologies-used&#34;&gt;Technologies Used
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Apache Airflow:&lt;/strong&gt; Orchestrates and schedules the tasks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python:&lt;/strong&gt; Implements the scraping logic using requests and BeautifulSoup.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PostgreSQL:&lt;/strong&gt; Stores the book data for further analysis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google Cloud Composer:&lt;/strong&gt; Manages the Airflow environment on Google Cloud.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terraform:&lt;/strong&gt; Automates the infrastructure setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setup-and-installation&#34;&gt;Setup and Installation
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This setup guide is intended for Windows users. If you&amp;rsquo;re using a different operating system, please refer to the official documentation for installation instructions specific to your platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites
&lt;/h3&gt;&lt;p&gt;Before starting the project setup, ensure you have the following tools and accounts configured:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Google Cloud Platform (GCP) Account&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A GCP account is required to create and manage cloud resources. If you don&amp;rsquo;t have an account, create one at &lt;a class=&#34;link&#34; href=&#34;https://cloud.google.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;cloud.google.com&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Google Cloud Project&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a new GCP project or use an existing one. Make a note of the project ID, as it will be used throughout the setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Terraform&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download and install Terraform from &lt;a class=&#34;link&#34; href=&#34;https://www.terraform.io/downloads.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;terraform.io&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Verify the installation by running:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform -v
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Google Cloud SDK (&lt;code&gt;gcloud&lt;/code&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install the Google Cloud SDK by following the instructions &lt;a class=&#34;link&#34; href=&#34;https://cloud.google.com/sdk/docs/install&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Initialize &lt;code&gt;gcloud&lt;/code&gt; and set the default project:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gcloud init
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gcloud config &lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt; project &amp;lt;YOUR_PROJECT_ID&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enable Required APIs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensure the following APIs are enabled in your GCP project:
&lt;ul&gt;
&lt;li&gt;Compute Engine API&lt;/li&gt;
&lt;li&gt;Cloud Composer API&lt;/li&gt;
&lt;li&gt;Cloud SQL Admin API&lt;/li&gt;
&lt;li&gt;Identity and Access Management (IAM) API&lt;/li&gt;
&lt;li&gt;Cloud Storage API&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Enable them using the &lt;code&gt;gcloud&lt;/code&gt; command:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gcloud services &lt;span class=&#34;nb&#34;&gt;enable&lt;/span&gt; compute.googleapis.com &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                       composer.googleapis.com &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                       sqladmin.googleapis.com &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                       iam.googleapis.com &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                       storage.googleapis.com
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-1-set-up-service-account&#34;&gt;Step 1: Set Up Service Account
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create a Service Account&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go to the &lt;a class=&#34;link&#34; href=&#34;https://console.cloud.google.com/iam-admin/serviceaccounts&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;IAM &amp;amp; Admin Console&lt;/a&gt; and create a new service account with the following details:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: &lt;code&gt;terraform-admin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Role&lt;/strong&gt;: &lt;code&gt;Owner&lt;/code&gt;, &lt;code&gt;roles/roleAdmin&lt;/code&gt;, &lt;code&gt;roles/iam.securityAdmin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generate a JSON Key&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After creating the service account, generate a JSON key and download it.&lt;/li&gt;
&lt;li&gt;Store the JSON key in a secure location, as it will be used by Terraform to authenticate with GCP.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Set Environment Variable for Authentication&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set the &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt; environment variable to the path of your service account key file:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/path/to/service-account-key.json&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-2-clone-the-repository-and-configure-terraform&#34;&gt;Step 2: Clone the Repository and Configure Terraform
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clone the Project Repository&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clone the repository containing the Terraform configurations and DAGs:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/imadharir/AmazonBookScaperETL.git
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; AmazonBookScaperETL
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Update &lt;code&gt;terraform.tfvars&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create or edit a &lt;code&gt;terraform.tfvars&lt;/code&gt; file in the root directory with your project-specific values:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-hcl&#34; data-lang=&#34;hcl&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;project_id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;region&lt;/span&gt;     &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;us-central1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;credentials_file&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/path/to/service-account-key.json&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initialize and Validate Terraform Configuration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the Terraform workspace:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform init
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Validate the configuration to ensure everything is set up correctly:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform validate
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-3-deploy-the-infrastructure&#34;&gt;Step 3: Deploy the Infrastructure
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deploy the Resources&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deploy the infrastructure using Terraform:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform apply
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Review the plan and confirm by typing &lt;code&gt;yes&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Verify the Deployment&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Check the GCP console to ensure that all the resources (e.g., Cloud Composer environment, Cloud SQL instance, VPC, etc.) have been created successfully.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-4-cloud-composer-configuration&#34;&gt;Step 4: Cloud Composer Configuration
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Configure Airflow Connections&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you didn&amp;rsquo;t create the Airflow connections using Terraform, go to the &lt;a class=&#34;link&#34; href=&#34;https://console.cloud.google.com/composer/environments&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cloud Composer UI&lt;/a&gt; and configure the necessary connections:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Postgres Connection&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conn Id&lt;/strong&gt;: &lt;code&gt;books_connection&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conn Type&lt;/strong&gt;: &lt;code&gt;Postgres&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Host&lt;/strong&gt;: &lt;code&gt;&amp;lt;Your Cloud SQL Internal IP&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Schema&lt;/strong&gt;: &lt;code&gt;amazon_books&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Login&lt;/strong&gt;: &lt;code&gt;postgres&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Password&lt;/strong&gt;: &lt;code&gt;&amp;lt;Your Password&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Port&lt;/strong&gt;: &lt;code&gt;5432&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Upload DAG Files&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upload the DAG file(s) to the &lt;code&gt;dags&lt;/code&gt; folder in the Cloud Composer bucket:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gsutil cp /local/path/to/dag.py gs://&amp;lt;COMPOSER_BUCKET_NAME&amp;gt;/dags/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-5-monitoring-and-logs&#34;&gt;Step 5: Monitoring and Logs
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Monitor the DAG Runs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Navigate to the &lt;a class=&#34;link&#34; href=&#34;https://console.cloud.google.com/composer/environments&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Airflow UI&lt;/a&gt; and monitor your DAG runs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-6-clean-up-resources&#34;&gt;Step 6: Clean Up Resources
&lt;/h3&gt;&lt;p&gt;To avoid incurring charges for unused resources, remember to delete the GCP project or individual resources when the project is no longer needed:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform destroy
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;usage&#34;&gt;Usage
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;The project runs on a daily basis, ensuring the database is updated with new book entries.&lt;/li&gt;
&lt;li&gt;Can be used for trend analysis, pricing insights, or recommendation systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repo&#34;&gt;Repo
&lt;/h2&gt;&lt;p&gt;👉 &lt;a class=&#34;link&#34; href=&#34;https://github.com/imadharir/AmazonBookScaperETL.git&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github Repository&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Pizza Sales Data Analytics</title>
        <link>https://imadharir.github.io/p/pizza-sales-data-analytics/</link>
        <pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>https://imadharir.github.io/p/pizza-sales-data-analytics/</guid>
        <description>&lt;img src="https://imadharir.github.io/p/pizza-sales-data-analytics/power-bi.png" alt="Featured image of post Pizza Sales Data Analytics" /&gt;&lt;h1 id=&#34;pizza-sales-analysis&#34;&gt;Pizza Sales Analysis
&lt;/h1&gt;&lt;h2 id=&#34;project-overview&#34;&gt;Project Overview
&lt;/h2&gt;&lt;p&gt;This project analyzes pizza sales data using Power BI to generate insights on revenue, order trends, and pizza category performance. The data covers the period from January 2015 to December 2015, focusing on metrics like total revenue, number of pizzas sold, and average order values.&lt;/p&gt;
&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement
&lt;/h2&gt;&lt;h3 id=&#34;business-challenge&#34;&gt;Business Challenge:
&lt;/h3&gt;&lt;p&gt;A pizza chain is experiencing inconsistent sales performance across its various locations and needs better insights into its sales patterns. The management team is struggling to understand which factors contribute to the variation in pizza sales, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which days and times generate the most sales?&lt;/li&gt;
&lt;li&gt;Which pizza categories and sizes perform best?&lt;/li&gt;
&lt;li&gt;Are there specific months where sales peak or decline?&lt;/li&gt;
&lt;li&gt;What are the top and bottom-selling pizzas, and how can the product mix be optimized?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Currently, decisions are being made based on intuition rather than data, resulting in lost opportunities for revenue optimization and efficient resource allocation.&lt;/p&gt;
&lt;h3 id=&#34;business-goals&#34;&gt;Business Goals:
&lt;/h3&gt;&lt;p&gt;The company needs a data-driven approach to answer these questions and identify trends to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimize their inventory and staffing based on high and low demand periods.&lt;/li&gt;
&lt;li&gt;Enhance their product offerings by focusing on high-selling pizzas.&lt;/li&gt;
&lt;li&gt;Create targeted marketing strategies around peak sales periods and best-selling products.&lt;/li&gt;
&lt;li&gt;Improve overall sales and profitability by focusing on underperforming areas.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution:
&lt;/h3&gt;&lt;p&gt;This project addresses the problem by analyzing the pizza sales data over a year, providing detailed insights into sales trends, best-selling pizzas, and customer preferences. The Power BI visualizations offer actionable intelligence, helping the pizza chain improve its operations, product offerings, and marketing strategies to enhance profitability.&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset
&lt;/h2&gt;&lt;p&gt;The dataset includes pizza sales information from January 1st, 2015 to December 31st, 2015. The main features of the dataset include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Total Revenue&lt;/strong&gt;: $817.86K&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Total Pizza Sold&lt;/strong&gt;: 49,574 pizzas&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Total Orders&lt;/strong&gt;: 21,350 orders&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average Pizzas per Order&lt;/strong&gt;: 2.32&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Average Order Value&lt;/strong&gt;: $38.31&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-insights&#34;&gt;Key Insights
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Busiest Days&lt;/strong&gt;: Orders are highest on weekends, particularly on Friday and Saturday evenings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best Months&lt;/strong&gt;: July and January have the most orders.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-Selling Pizza&lt;/strong&gt;: Thai Chicken Pizza generates the highest revenue, while Classic Deluxe Pizza has the highest total quantity sold.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Worst-Selling Pizza&lt;/strong&gt;: Brie Carre Pizza ranks lowest in terms of revenue, total quantity sold, and total orders.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pizza-category-sales&#34;&gt;Pizza Category Sales
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Classic Category&lt;/strong&gt; contributes the most to total sales.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large pizzas&lt;/strong&gt; generate the highest revenue across all size categories.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sales-performance-by-pizza-size&#34;&gt;Sales Performance by Pizza Size
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Large&lt;/strong&gt; size pizzas are the best-sellers by both quantity and revenue.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;visualizations&#34;&gt;Visualizations
&lt;/h2&gt;&lt;p&gt;The analysis includes the following Power BI visuals:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Daily Trend for Total Orders&lt;/strong&gt;: Shows the fluctuation in daily orders over the week.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monthly Trend for Total Orders&lt;/strong&gt;: Displays the variation in orders across months.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sales by Pizza Category&lt;/strong&gt;: Breakdown of sales percentage by pizza category.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sales by Pizza Size&lt;/strong&gt;: Breakdown of sales percentage by pizza size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Top 5 and Bottom 5 Pizza Sellers&lt;/strong&gt;: A list of the best and worst performing pizzas in terms of revenue, quantity, and total orders.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&#34;dash1.png&#34; alt=&#34;Dashboard Page 1&#34; width=&#34;400&#34;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&#34;dash2.png&#34; alt=&#34;Dashboard Page 2&#34; width=&#34;400&#34;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id=&#34;tools-used&#34;&gt;Tools Used
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Power BI&lt;/strong&gt;: Used for data visualization and generating insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CSV Dataset&lt;/strong&gt;: Raw sales data was provided in a CSV format.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-improvements&#34;&gt;Future Improvements
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Include more advanced sales forecasting models.&lt;/li&gt;
&lt;li&gt;Add customer demographic data for better segmentation and targeted marketing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repo&#34;&gt;Repo
&lt;/h2&gt;&lt;p&gt;👉 &lt;a class=&#34;link&#34; href=&#34;https://github.com/imadharir/Pizza-Sales-Data-Analysis.git&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github Repository&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Stock Market Real Time Data Analysis</title>
        <link>https://imadharir.github.io/p/stock-market-real-time-data-analysis/</link>
        <pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>https://imadharir.github.io/p/stock-market-real-time-data-analysis/</guid>
        <description>&lt;img src="https://imadharir.github.io/p/stock-market-real-time-data-analysis/stockMarketDiagram.jpg" alt="Featured image of post Stock Market Real Time Data Analysis" /&gt;&lt;h1 id=&#34;google-cloud-pubsub-infrastructure-with-terraform&#34;&gt;Google Cloud Pub/Sub Infrastructure with Terraform
&lt;/h1&gt;&lt;p&gt;  This project is focused on building a data pipeline for real-time stock market data analysis using Google Cloud Platform (GCP) and other open-source technologies. The pipeline ingests, processes, and analyzes stock market data to provide valuable insights.
  This project automates the deployment of Google Cloud Pub/Sub resources using Terraform. The infrastructure includes a Pub/Sub topic and subscription, allowing for messaging between different components of the system.&lt;/p&gt;
&lt;h2 id=&#34;project-architecture&#34;&gt;Project Architecture
&lt;/h2&gt;&lt;p&gt;Below is the architecture of the project:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://imadharir.github.io/p/stock-market-real-time-data-analysis/stockMarketDiagram.jpg&#34;
	width=&#34;815&#34;
	height=&#34;586&#34;
	srcset=&#34;https://imadharir.github.io/p/stock-market-real-time-data-analysis/stockMarketDiagram_hu4688469147005051992.jpg 480w, https://imadharir.github.io/p/stock-market-real-time-data-analysis/stockMarketDiagram_hu7197408719458081956.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;139&#34;
		data-flex-basis=&#34;333px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;resources-deployed&#34;&gt;Resources Deployed
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pub/Sub Topic&lt;/strong&gt;: A topic for sending messages.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pub/Sub Subscription&lt;/strong&gt;: A subscription to recpip install dotenveive messages from the topic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud Storage Bucket&lt;/strong&gt;: A general-purpose bucket for data storage and ingestion.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud Function Storage Bucket&lt;/strong&gt;: A bucket to store the zip file containing the Cloud Function code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud Function&lt;/strong&gt;: A serverless function triggered by Cloud Storage events to process data and load it into BigQuery.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BigQuery Dataset&lt;/strong&gt;: A dataset for organizing tables in BigQuery.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BigQuery Table&lt;/strong&gt;: A table for storing stock market data, with schema auto-detected.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-components&#34;&gt;Key Components
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;main.tf&lt;/code&gt;&lt;/strong&gt;: The entry point for the Terraform configuration, defining the provider (Google Cloud) and project-level settings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;pubsub.tf&lt;/code&gt;&lt;/strong&gt;: Contains the configuration for creating the Pub/Sub topic and subscription.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;function.tf&lt;/code&gt;&lt;/strong&gt;: Defines the setup for the Google Cloud Function that processes data from Cloud Storage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;functionBucket.tf&lt;/code&gt;&lt;/strong&gt;: Manages the Cloud Storage bucket specifically for storing Cloud Function code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;dataset.tf&lt;/code&gt;&lt;/strong&gt;: Creates the BigQuery dataset that holds the table for stock market data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;bq_table&lt;/code&gt;&lt;/strong&gt;.tf: Defines the BigQuery table for storing data, with schema auto-detection enabled.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;bucket.tf&lt;/code&gt;&lt;/strong&gt;: Defines another bucket for Cloud Storage, used for data ingestion purposes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;variables.tf&lt;/code&gt;&lt;/strong&gt;: Defines variables such as project ID, region, topic name, and subscription name to make the configuration flexible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;terraform.tfvars&lt;/code&gt;&lt;/strong&gt;: This file contains the values for the variables defined in &lt;code&gt;variables.tf&lt;/code&gt;, making it easy to customize for different environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cloud-function-overview&#34;&gt;Cloud Function Overview
&lt;/h2&gt;&lt;p&gt;The Cloud Function is triggered when a JSON file is uploaded to the Cloud Storage bucket. It loads the data into BigQuery, automatically detecting the schema of the JSON file.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.terraform.io/downloads&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Terraform&lt;/a&gt; v1.0.0 or later&lt;/li&gt;
&lt;li&gt;A Google Cloud account&lt;/li&gt;
&lt;li&gt;A Google Cloud project with billing enabled&lt;/li&gt;
&lt;li&gt;Google Cloud SDK installed on your local machine&lt;/li&gt;
&lt;li&gt;A service account with appropriate permissions (Pub/Sub, Cloud Functions, BigQuery, Storage)&lt;/li&gt;
&lt;li&gt;Zip file containing the Cloud Function code (main.py and requirements.txt)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-use&#34;&gt;How to Use
&lt;/h2&gt;&lt;h3 id=&#34;terraform-setup&#34;&gt;Terraform Setup
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/imadharir/gcp-data-pipeline-stock-market.git
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; gcp-data-pipeline-stock-market
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Terraform&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Set up a virtual environment&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python3 -m venv venv
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;venv&lt;span class=&#34;se&#34;&gt;\S&lt;/span&gt;cripts&lt;span class=&#34;se&#34;&gt;\a&lt;/span&gt;ctivate
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -r requirements.txt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Grant permissions to the Google Cloud Function service account&lt;/strong&gt;:
Ensure that the service account used by Google Cloud Function has the necessary permissions to access Pub/Sub, BigQuery, and Cloud Storage.
You can do this by adding the following roles to the service account: &lt;code&gt;Pub/Sub Publisher&lt;/code&gt;, &lt;code&gt;Pub/Sub Subscriber&lt;/code&gt;, &lt;code&gt;BigQuery Data Editor&lt;/code&gt;, &lt;code&gt;Storage Object Admin&lt;/code&gt;.
The default service account is typically:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;PROJECT_ID&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;@appspot.gserviceaccount.com
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prepare the environment&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;path/to/your-service-account-key.json&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initialize Terraform&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform init
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Plan the infrasctructure&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform plan
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Review the changes that Terraform will make.&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apply the infrastructure&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform apply
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Confirm the application by typing &amp;lsquo;yes&amp;rsquo; when prompted.&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Upload the Cloud Function zip file&lt;/strong&gt;:
Ensure the zip file containing the Cloud Function code (main.py, requirements.txt) is in the correct path and gets uploaded to the specified bucket.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dbt-setup&#34;&gt;dbt Setup
&lt;/h3&gt;&lt;p&gt;This section describes the integration of dbt (Data Build Tool) for data transformation and modeling within the project.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Set &lt;code&gt;DBT_PROFILES_DIR&lt;/code&gt; environment variable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change directory:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; dbt_integration
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Modify the file &lt;code&gt;profiles.yml&lt;/code&gt; with your own connection credentials:&lt;/strong&gt;
A &lt;code&gt;profiles.yml&lt;/code&gt; file should be configured with the necessary details for connecting to the BigQuery data warehouse.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set up Application Default Credentials:&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gcloud init
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gcloud auth application-default login
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Run dbt commands:&lt;/strong&gt;
&lt;strong&gt;Models&lt;/strong&gt;: The &lt;code&gt;myModel.sql&lt;/code&gt; file contains the transformations applied to the stock market data to clean, aggregate, and enhance the dataset.
&lt;strong&gt;Tests&lt;/strong&gt;: Custom tests, such as &lt;code&gt;not_null_all_columns.sql&lt;/code&gt;, have been implemented to validate the data integrity and ensure no null values are present in the critical columns.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dbt run
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dbt &lt;span class=&#34;nb&#34;&gt;test&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Or&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dbt build
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;cleanup&#34;&gt;Cleanup
&lt;/h2&gt;&lt;p&gt;To delete the resources created by Terraform, you can run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;terraform destroy
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;repo&#34;&gt;Repo
&lt;/h2&gt;&lt;p&gt;👉 &lt;a class=&#34;link&#34; href=&#34;https://github.com/imadharir/gcp-data-pipeline-stock-market.git&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github Repository&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Development of a Multi-Horizon Forecasting Tool for National Electricity Demand Based on Artificial Intelligence</title>
        <link>https://imadharir.github.io/p/development-of-a-multi-horizon-forecasting-tool-for-national-electricity-demand-based-on-artificial-intelligence/</link>
        <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>https://imadharir.github.io/p/development-of-a-multi-horizon-forecasting-tool-for-national-electricity-demand-based-on-artificial-intelligence/</guid>
        <description>&lt;img src="https://imadharir.github.io/p/development-of-a-multi-horizon-forecasting-tool-for-national-electricity-demand-based-on-artificial-intelligence/app.png" alt="Featured image of post Development of a Multi-Horizon Forecasting Tool for National Electricity Demand Based on Artificial Intelligence" /&gt;&lt;h2 id=&#34;project-title&#34;&gt;Project Title:
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Development of a Multi-Horizon Forecasting Tool for National Electricity Demand Based on Artificial Intelligence&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-description&#34;&gt;Project Description:
&lt;/h2&gt;&lt;p&gt;This project aims to develop a multi-horizon forecasting tool that predicts the national electricity demand using artificial intelligence (AI) techniques. The forecasting tool is designed to enhance the management of electricity supply by providing more accurate and reliable demand predictions over different time horizons.&lt;/p&gt;
&lt;h2 id=&#34;key-objectives&#34;&gt;Key Objectives:
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accurate Demand Forecasting&lt;/strong&gt;: To build a model that predicts electricity demand over short, medium-term horizons.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI Integration&lt;/strong&gt;: Leverage advanced machine learning techniques to handle complex and dynamic energy consumption patterns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: Ensure efficient use of national energy resources by minimizing forecasting errors, thereby supporting better decision-making in energy distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology:
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;: Historical electricity consumption data was collected from the national grid.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modeling Techniques&lt;/strong&gt;: The project employed various AI models including, but not limited to, neural networks and time series analysis (ARIMA, LSTM, GRU).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Horizon Approach&lt;/strong&gt;: The model was designed to provide predictions at different intervals (e.g., hourly, daily, and weekly) to support both immediate and medium-term planning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tools-and-technologies-used&#34;&gt;Tools and Technologies Used:
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Programming Languages&lt;/strong&gt;: Python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Libraries &amp;amp; Frameworks&lt;/strong&gt;: TensorFlow, Scikit-learn, Pandas, NumPy&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Visualization&lt;/strong&gt;: Matplotlib, Seaborn for performance analysis and model insights&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results:
&lt;/h2&gt;&lt;p&gt;The developed tool demonstrated improved accuracy in electricity demand predictions compared to traditional forecasting methods. The multi-horizon approach provided flexibility in adjusting to various operational needs, from short-term demand spikes to medmiu-term resource planning.&lt;/p&gt;
&lt;h2 id=&#34;future-improvements&#34;&gt;Future Improvements:
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Expanding the model to include renewable energy data and integrate weather-related factors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-time Data Integration&lt;/strong&gt;: Incorporate real-time data feeds to enhance the adaptability of the forecasts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Further AI Techniques&lt;/strong&gt;: Exploring other AI and deep learning models to further refine prediction accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage:
&lt;/h2&gt;&lt;p&gt;The tool is designed for use by energy planners and decision-makers at &lt;strong&gt;Office National d’Électricité et d’Eau Potable&lt;/strong&gt;. It can be integrated into existing systems for continuous monitoring and adjustment of energy supply to meet the national demand more efficiently.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Product Management Application</title>
        <link>https://imadharir.github.io/p/product-management-application/</link>
        <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
        
        <guid>https://imadharir.github.io/p/product-management-application/</guid>
        <description>&lt;img src="https://imadharir.github.io/p/product-management-application/list_products.png" alt="Featured image of post Product Management Application" /&gt;&lt;h1 id=&#34;product-management-application&#34;&gt;Product Management Application
&lt;/h1&gt;&lt;h2 id=&#34;1-overview&#34;&gt;1. Overview
&lt;/h2&gt;&lt;p&gt;This project is a web application built using Flask that allows users to manage products. It includes functionalities to add, update, delete, and view products and their associated categories. The application stores data in a Cassandra database and supports image uploads for product representation.&lt;/p&gt;
&lt;h2 id=&#34;2-features&#34;&gt;2. Features
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Product Management&lt;/strong&gt;: Add, update, and delete products.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Category Management&lt;/strong&gt;: Automatically add categories if they do not exist.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Image Upload&lt;/strong&gt;: Upload and store product images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Responsive Design&lt;/strong&gt;: Uses Flask-Bootstrap for a responsive layout.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RESTful API&lt;/strong&gt;: Endpoint to retrieve categories in JSON format.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-technologies-used&#34;&gt;3. Technologies Used
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Flask&lt;/strong&gt;: Web framework for building the application.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flask-Bootstrap&lt;/strong&gt;: For styling the application using Bootstrap.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cassandra&lt;/strong&gt;: NoSQL database for storing product and category data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pillow&lt;/strong&gt;: Python Imaging Library for image handling.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-installation-instructions&#34;&gt;4. Installation Instructions
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Clone the Repository:&lt;/strong&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/imadharir/Product-Management-Application.git
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; Product-Management-Application
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set Up a Virtual Environment:&lt;/strong&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python3 -m venv venv
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt; venv/bin/activate  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install Dependencies:&lt;/strong&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install Flask flask_bootstrap cassandra-driver Pillow
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set Up Cassandra:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Make sure Cassandra is installed and running.&lt;/li&gt;
&lt;li&gt;Create the keyspace and tables in the Cassandra CQL shell:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CREATE KEYSPACE mykeyspace WITH &lt;span class=&#34;nv&#34;&gt;replication&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;class&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;SimpleStrategy&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;replication_factor&amp;#39;&lt;/span&gt;: &lt;span class=&#34;s1&#34;&gt;&amp;#39;1&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; USE mykeyspace&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; CREATE TABLE products &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     product_id int PRIMARY KEY,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     name text,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     price float,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     category text,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     image_url text
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; CREATE TABLE categories &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     category text PRIMARY KEY
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;5-running-the-application&#34;&gt;5. Running the Application
&lt;/h2&gt;&lt;p&gt;To run the application, execute the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python app.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Visit &lt;code&gt;http://127.0.0.1:5000/&lt;/code&gt; in your web browser to access the application.&lt;/p&gt;
&lt;h2 id=&#34;6-usage&#34;&gt;6. Usage:
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Home Page:&lt;/strong&gt; View all products and their details.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Add Product:&lt;/strong&gt; Fill out the form to add a new product with an image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update Product:&lt;/strong&gt; Update the price of an existing product.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delete Product:&lt;/strong&gt; Remove a product from the list.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;View Categories:&lt;/strong&gt; Access the categories through the &lt;code&gt;/categories&lt;/code&gt; API endpoint.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;7-api-endpoints&#34;&gt;7. API Endpoints
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;GET /categories&lt;/strong&gt;: Retrieve a list of all categories in JSON format.&lt;/p&gt;
&lt;h2 id=&#34;8-screenshots&#34;&gt;8. Screenshots
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://imadharir.github.io/p/product-management-application/list_products.png&#34;
	width=&#34;1292&#34;
	height=&#34;592&#34;
	srcset=&#34;https://imadharir.github.io/p/product-management-application/list_products_hu17421241949092634519.png 480w, https://imadharir.github.io/p/product-management-application/list_products_hu5745717231803895927.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;218&#34;
		data-flex-basis=&#34;523px&#34;
	
&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
    &lt;hr style=&#34;width: 50%;&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://imadharir.github.io/p/product-management-application/add_product.png&#34;
	width=&#34;1292&#34;
	height=&#34;526&#34;
	srcset=&#34;https://imadharir.github.io/p/product-management-application/add_product_hu11835175880416828179.png 480w, https://imadharir.github.io/p/product-management-application/add_product_hu16533019504229596874.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;245&#34;
		data-flex-basis=&#34;589px&#34;
	
&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
    &lt;hr style=&#34;width: 50%;&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://imadharir.github.io/p/product-management-application/product_details.png&#34;
	width=&#34;1287&#34;
	height=&#34;598&#34;
	srcset=&#34;https://imadharir.github.io/p/product-management-application/product_details_hu14635157025304476134.png 480w, https://imadharir.github.io/p/product-management-application/product_details_hu17068863261404060800.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;516px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;9-future-improvements&#34;&gt;9. Future Improvements
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Implement user authentication and authorization.&lt;/li&gt;
&lt;li&gt;Add search functionality for products.&lt;/li&gt;
&lt;li&gt;Enhance error handling and validation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;repo&#34;&gt;Repo
&lt;/h2&gt;&lt;p&gt;👉 &lt;a class=&#34;link&#34; href=&#34;https://github.com/imadharir/Product-Management-Application.git&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github Repository&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
