[{"content":"Amazon Book Scraper and Storage\rProject Overview\rThe Amazon Book Scraper and Storage project is an automated data pipeline that scrapes book information from Amazon and stores it in a PostgreSQL database. The project leverages Apache Airflow for orchestration and scheduling of the scraping and data ingestion process.\nTable of Contents\rProject Overview\rArchitecture\rFeatures\rTechnologies Used\rSetup and Installation\rUsage\rArchitecture\rThe project follows a simple ETL (Extract, Transform, Load) architecture:\rExtract: Data is scraped from Amazon using a Python script with requests and BeautifulSoup. Transform: The extracted data is cleaned and structured using pandas. Load: The structured data is inserted into a PostgreSQL database using airflow.providers.postgres.hooks.postgres. High-Level Architecture Components:\rAirflow DAG: Manages and schedules the daily data pipeline tasks. Python Scraper: Extracts book data (title, author, price, rating) from Amazon. PostgreSQL Database: Stores the scraped book data. Google Cloud Composer: Manages the Airflow orchestration in a cloud environment. Features\rAutomated Daily Scraping: The pipeline runs daily, automatically scraping new book data. Data Deduplication: Ensures only new books are added during each subsequent run. Cloud-Based Orchestration: Uses Google Cloud Composer for cloud-based execution and management. Modular Design: Components are decoupled, making it easy to extend or modify parts of the pipeline. Technologies Used\rApache Airflow: Orchestrates and schedules the tasks. Python: Implements the scraping logic using requests and BeautifulSoup. PostgreSQL: Stores the book data for further analysis. Google Cloud Composer: Manages the Airflow environment on Google Cloud. Terraform: Automates the infrastructure setup. Setup and Installation\rNote: This setup guide is intended for Windows users. If you\u0026rsquo;re using a different operating system, please refer to the official documentation for installation instructions specific to your platform.\nPrerequisites\rBefore starting the project setup, ensure you have the following tools and accounts configured:\nGoogle Cloud Platform (GCP) Account\nA GCP account is required to create and manage cloud resources. If you don\u0026rsquo;t have an account, create one at cloud.google.com. Google Cloud Project\nCreate a new GCP project or use an existing one. Make a note of the project ID, as it will be used throughout the setup. Install Terraform\nDownload and install Terraform from terraform.io. Verify the installation by running: 1 terraform -v Install Google Cloud SDK (gcloud)\nInstall the Google Cloud SDK by following the instructions here. Initialize gcloud and set the default project: 1 2 gcloud init gcloud config set project \u0026lt;YOUR_PROJECT_ID\u0026gt; Enable Required APIs\nEnsure the following APIs are enabled in your GCP project: Compute Engine API Cloud Composer API Cloud SQL Admin API Identity and Access Management (IAM) API Cloud Storage API Enable them using the gcloud command: 1 2 3 4 5 gcloud services enable compute.googleapis.com \\ composer.googleapis.com \\ sqladmin.googleapis.com \\ iam.googleapis.com \\ storage.googleapis.com Step 1: Set Up Service Account\rCreate a Service Account\nGo to the IAM \u0026amp; Admin Console and create a new service account with the following details: Name: terraform-admin Role: Owner, roles/roleAdmin, roles/iam.securityAdmin Generate a JSON Key\nAfter creating the service account, generate a JSON key and download it. Store the JSON key in a secure location, as it will be used by Terraform to authenticate with GCP. Set Environment Variable for Authentication\nSet the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your service account key file: 1 export GOOGLE_APPLICATION_CREDENTIALS=\u0026#34;/path/to/service-account-key.json\u0026#34; Step 2: Clone the Repository and Configure Terraform\rClone the Project Repository\nClone the repository containing the Terraform configurations and DAGs: 1 2 git clone https://github.com/imadharir/AmazonBookScaperETL.git cd AmazonBookScaperETL Update terraform.tfvars\nCreate or edit a terraform.tfvars file in the root directory with your project-specific values: 1 2 3 project_id = \u0026#34;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026#34; region = \u0026#34;us-central1\u0026#34; credentials_file = \u0026#34;/path/to/service-account-key.json\u0026#34; Initialize and Validate Terraform Configuration\nInitialize the Terraform workspace: 1 terraform init Validate the configuration to ensure everything is set up correctly: 1 terraform validate Step 3: Deploy the Infrastructure\rDeploy the Resources\nDeploy the infrastructure using Terraform: 1 terraform apply Review the plan and confirm by typing yes. Verify the Deployment\nCheck the GCP console to ensure that all the resources (e.g., Cloud Composer environment, Cloud SQL instance, VPC, etc.) have been created successfully. Step 4: Cloud Composer Configuration\rConfigure Airflow Connections\nIf you didn\u0026rsquo;t create the Airflow connections using Terraform, go to the Cloud Composer UI and configure the necessary connections: Postgres Connection: Conn Id: books_connection Conn Type: Postgres Host: \u0026lt;Your Cloud SQL Internal IP\u0026gt; Schema: amazon_books Login: postgres Password: \u0026lt;Your Password\u0026gt; Port: 5432 Upload DAG Files\nUpload the DAG file(s) to the dags folder in the Cloud Composer bucket: 1 gsutil cp /local/path/to/dag.py gs://\u0026lt;COMPOSER_BUCKET_NAME\u0026gt;/dags/ Step 5: Monitoring and Logs\rMonitor the DAG Runs\nNavigate to the Airflow UI and monitor your DAG runs. Step 6: Clean Up Resources\rTo avoid incurring charges for unused resources, remember to delete the GCP project or individual resources when the project is no longer needed:\n1 terraform destroy Usage\rThe project runs on a daily basis, ensuring the database is updated with new book entries. Can be used for trend analysis, pricing insights, or recommendation systems. Repo\rðŸ‘‰ Github Repository\n","date":"2024-09-30T00:00:00Z","image":"https://imadharir.github.io/p/amazon-books-scraper/AmazonBookScraper_hu3766364554707935124.png","permalink":"https://imadharir.github.io/p/amazon-books-scraper/","title":"Amazon Books Scraper"},{"content":"Pizza Sales Analysis\rProject Overview\rThis project analyzes pizza sales data using Power BI to generate insights on revenue, order trends, and pizza category performance. The data covers the period from January 2015 to December 2015, focusing on metrics like total revenue, number of pizzas sold, and average order values.\nProblem Statement\rBusiness Challenge:\rA pizza chain is experiencing inconsistent sales performance across its various locations and needs better insights into its sales patterns. The management team is struggling to understand which factors contribute to the variation in pizza sales, such as:\nWhich days and times generate the most sales? Which pizza categories and sizes perform best? Are there specific months where sales peak or decline? What are the top and bottom-selling pizzas, and how can the product mix be optimized? Currently, decisions are being made based on intuition rather than data, resulting in lost opportunities for revenue optimization and efficient resource allocation.\nBusiness Goals:\rThe company needs a data-driven approach to answer these questions and identify trends to:\nOptimize their inventory and staffing based on high and low demand periods. Enhance their product offerings by focusing on high-selling pizzas. Create targeted marketing strategies around peak sales periods and best-selling products. Improve overall sales and profitability by focusing on underperforming areas. Solution:\rThis project addresses the problem by analyzing the pizza sales data over a year, providing detailed insights into sales trends, best-selling pizzas, and customer preferences. The Power BI visualizations offer actionable intelligence, helping the pizza chain improve its operations, product offerings, and marketing strategies to enhance profitability.\nDataset\rThe dataset includes pizza sales information from January 1st, 2015 to December 31st, 2015. The main features of the dataset include:\nTotal Revenue: $817.86K Total Pizza Sold: 49,574 pizzas Total Orders: 21,350 orders Average Pizzas per Order: 2.32 Average Order Value: $38.31 Key Insights\rBusiest Days: Orders are highest on weekends, particularly on Friday and Saturday evenings. Best Months: July and January have the most orders. Best-Selling Pizza: Thai Chicken Pizza generates the highest revenue, while Classic Deluxe Pizza has the highest total quantity sold. Worst-Selling Pizza: Brie Carre Pizza ranks lowest in terms of revenue, total quantity sold, and total orders. Pizza Category Sales\rClassic Category contributes the most to total sales. Large pizzas generate the highest revenue across all size categories. Sales Performance by Pizza Size\rLarge size pizzas are the best-sellers by both quantity and revenue. Visualizations\rThe analysis includes the following Power BI visuals:\nDaily Trend for Total Orders: Shows the fluctuation in daily orders over the week. Monthly Trend for Total Orders: Displays the variation in orders across months. Sales by Pizza Category: Breakdown of sales percentage by pizza category. Sales by Pizza Size: Breakdown of sales percentage by pizza size. Top 5 and Bottom 5 Pizza Sellers: A list of the best and worst performing pizzas in terms of revenue, quantity, and total orders. Tools Used\rPower BI: Used for data visualization and generating insights. CSV Dataset: Raw sales data was provided in a CSV format. Future Improvements\rInclude more advanced sales forecasting models. Add customer demographic data for better segmentation and targeted marketing. Repo\rðŸ‘‰ Github Repository\n","date":"2024-09-30T00:00:00Z","image":"https://imadharir.github.io/p/pizza-sales-data-analytics/power-bi_hu5442386931287793823.png","permalink":"https://imadharir.github.io/p/pizza-sales-data-analytics/","title":"Pizza Sales Data Analytics"},{"content":"Google Cloud Pub/Sub Infrastructure with Terraform\rThis project is focused on building a data pipeline for real-time stock market data analysis using Google Cloud Platform (GCP) and other open-source technologies. The pipeline ingests, processes, and analyzes stock market data to provide valuable insights. This project automates the deployment of Google Cloud Pub/Sub resources using Terraform. The infrastructure includes a Pub/Sub topic and subscription, allowing for messaging between different components of the system.\nProject Architecture\rBelow is the architecture of the project:\nResources Deployed\rPub/Sub Topic: A topic for sending messages. Pub/Sub Subscription: A subscription to recpip install dotenveive messages from the topic. Cloud Storage Bucket: A general-purpose bucket for data storage and ingestion. Cloud Function Storage Bucket: A bucket to store the zip file containing the Cloud Function code. Cloud Function: A serverless function triggered by Cloud Storage events to process data and load it into BigQuery. BigQuery Dataset: A dataset for organizing tables in BigQuery. BigQuery Table: A table for storing stock market data, with schema auto-detected. Key Components\rmain.tf: The entry point for the Terraform configuration, defining the provider (Google Cloud) and project-level settings. pubsub.tf: Contains the configuration for creating the Pub/Sub topic and subscription. function.tf: Defines the setup for the Google Cloud Function that processes data from Cloud Storage. functionBucket.tf: Manages the Cloud Storage bucket specifically for storing Cloud Function code. dataset.tf: Creates the BigQuery dataset that holds the table for stock market data. bq_table.tf: Defines the BigQuery table for storing data, with schema auto-detection enabled. bucket.tf: Defines another bucket for Cloud Storage, used for data ingestion purposes. variables.tf: Defines variables such as project ID, region, topic name, and subscription name to make the configuration flexible. terraform.tfvars: This file contains the values for the variables defined in variables.tf, making it easy to customize for different environments. Cloud Function Overview\rThe Cloud Function is triggered when a JSON file is uploaded to the Cloud Storage bucket. It loads the data into BigQuery, automatically detecting the schema of the JSON file.\nPrerequisites\rTerraform v1.0.0 or later A Google Cloud account A Google Cloud project with billing enabled Google Cloud SDK installed on your local machine A service account with appropriate permissions (Pub/Sub, Cloud Functions, BigQuery, Storage) Zip file containing the Cloud Function code (main.py and requirements.txt) How to Use\rTerraform Setup\rClone the repository:\n1 2 git clone https://github.com/imadharir/gcp-data-pipeline-stock-market.git cd gcp-data-pipeline-stock-market Install Terraform\nSet up a virtual environment:\n1 2 python3 -m venv venv venv\\Scripts\\activate Install dependencies::\n1 pip install -r requirements.txt Grant permissions to the Google Cloud Function service account: Ensure that the service account used by Google Cloud Function has the necessary permissions to access Pub/Sub, BigQuery, and Cloud Storage. You can do this by adding the following roles to the service account: Pub/Sub Publisher, Pub/Sub Subscriber, BigQuery Data Editor, Storage Object Admin. The default service account is typically:\n1 [PROJECT_ID]@appspot.gserviceaccount.com Prepare the environment:\n1 export GOOGLE_APPLICATION_CREDENTIALS=\u0026#34;path/to/your-service-account-key.json\u0026#34; Initialize Terraform:\n1 terraform init Plan the infrasctructure:\n1 terraform plan Review the changes that Terraform will make.\nApply the infrastructure:\n1 terraform apply Confirm the application by typing \u0026lsquo;yes\u0026rsquo; when prompted.\nUpload the Cloud Function zip file: Ensure the zip file containing the Cloud Function code (main.py, requirements.txt) is in the correct path and gets uploaded to the specified bucket.\ndbt Setup\rThis section describes the integration of dbt (Data Build Tool) for data transformation and modeling within the project.\nSet DBT_PROFILES_DIR environment variable Change directory: 1 cd dbt_integration Modify the file profiles.yml with your own connection credentials: A profiles.yml file should be configured with the necessary details for connecting to the BigQuery data warehouse. Set up Application Default Credentials: 1 2 gcloud init gcloud auth application-default login Run dbt commands: Models: The myModel.sql file contains the transformations applied to the stock market data to clean, aggregate, and enhance the dataset. Tests: Custom tests, such as not_null_all_columns.sql, have been implemented to validate the data integrity and ensure no null values are present in the critical columns. 1 2 dbt run dbt test Or\n1 dbt build Cleanup\rTo delete the resources created by Terraform, you can run:\n1 terraform destroy Repo\rðŸ‘‰ Github Repository\n","date":"2024-09-30T00:00:00Z","image":"https://imadharir.github.io/p/stock-market-real-time-data-analysis/stockMarketDiagram_hu4407290405579220816.jpg","permalink":"https://imadharir.github.io/p/stock-market-real-time-data-analysis/","title":"Stock Market Real Time Data Analysis"},{"content":"Project Title:\rDevelopment of a Multi-Horizon Forecasting Tool for National Electricity Demand Based on Artificial Intelligence\nProject Description:\rThis project aims to develop a multi-horizon forecasting tool that predicts the national electricity demand using artificial intelligence (AI) techniques. The forecasting tool is designed to enhance the management of electricity supply by providing more accurate and reliable demand predictions over different time horizons.\nKey Objectives:\rAccurate Demand Forecasting: To build a model that predicts electricity demand over short, medium-term horizons. AI Integration: Leverage advanced machine learning techniques to handle complex and dynamic energy consumption patterns. Optimization: Ensure efficient use of national energy resources by minimizing forecasting errors, thereby supporting better decision-making in energy distribution. Methodology:\rData Collection: Historical electricity consumption data was collected from the national grid. Modeling Techniques: The project employed various AI models including, but not limited to, neural networks and time series analysis (ARIMA, LSTM, GRU). Multi-Horizon Approach: The model was designed to provide predictions at different intervals (e.g., hourly, daily, and weekly) to support both immediate and medium-term planning. Tools and Technologies Used:\rProgramming Languages: Python Libraries \u0026amp; Frameworks: TensorFlow, Scikit-learn, Pandas, NumPy Data Visualization: Matplotlib, Seaborn for performance analysis and model insights Results:\rThe developed tool demonstrated improved accuracy in electricity demand predictions compared to traditional forecasting methods. The multi-horizon approach provided flexibility in adjusting to various operational needs, from short-term demand spikes to medmiu-term resource planning.\nFuture Improvements:\rScalability: Expanding the model to include renewable energy data and integrate weather-related factors. Real-time Data Integration: Incorporate real-time data feeds to enhance the adaptability of the forecasts. Further AI Techniques: Exploring other AI and deep learning models to further refine prediction accuracy. Usage:\rThe tool is designed for use by energy planners and decision-makers at Office National dâ€™Ã‰lectricitÃ© et dâ€™Eau Potable. It can be integrated into existing systems for continuous monitoring and adjustment of energy supply to meet the national demand more efficiently.\n","date":"2024-06-15T00:00:00Z","image":"https://imadharir.github.io/p/development-of-a-multi-horizon-forecasting-tool-for-national-electricity-demand-based-on-artificial-intelligence/app_hu12857224253122050039.png","permalink":"https://imadharir.github.io/p/development-of-a-multi-horizon-forecasting-tool-for-national-electricity-demand-based-on-artificial-intelligence/","title":"Development of a Multi-Horizon Forecasting Tool for National Electricity Demand Based on Artificial Intelligence"},{"content":"Product Management Application\r1. Overview\rThis project is a web application built using Flask that allows users to manage products. It includes functionalities to add, update, delete, and view products and their associated categories. The application stores data in a Cassandra database and supports image uploads for product representation.\n2. Features\rProduct Management: Add, update, and delete products. Category Management: Automatically add categories if they do not exist. Image Upload: Upload and store product images. Responsive Design: Uses Flask-Bootstrap for a responsive layout. RESTful API: Endpoint to retrieve categories in JSON format. 3. Technologies Used\rFlask: Web framework for building the application. Flask-Bootstrap: For styling the application using Bootstrap. Cassandra: NoSQL database for storing product and category data. Pillow: Python Imaging Library for image handling. 4. Installation Instructions\rClone the Repository: 1 2 git clone https://github.com/imadharir/Product-Management-Application.git cd Product-Management-Application Set Up a Virtual Environment: 1 2 python3 -m venv venv source venv/bin/activate Install Dependencies: 1 pip install Flask flask_bootstrap cassandra-driver Pillow Set Up Cassandra: Make sure Cassandra is installed and running. Create the keyspace and tables in the Cassandra CQL shell: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE KEYSPACE mykeyspace WITH replication = {\u0026#39;class\u0026#39;: \u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39;: \u0026#39;1\u0026#39;}; USE mykeyspace; CREATE TABLE products ( product_id int PRIMARY KEY, name text, price float, category text, image_url text ); CREATE TABLE categories ( category text PRIMARY KEY ); 5. Running the Application\rTo run the application, execute the following command:\n1 python app.py Visit http://127.0.0.1:5000/ in your web browser to access the application.\n6. Usage:\rHome Page: View all products and their details. Add Product: Fill out the form to add a new product with an image. Update Product: Update the price of an existing product. Delete Product: Remove a product from the list. View Categories: Access the categories through the /categories API endpoint. 7. API Endpoints\rGET /categories: Retrieve a list of all categories in JSON format.\n8. Screenshots\r9. Future Improvements\rImplement user authentication and authorization. Add search functionality for products. Enhance error handling and validation. Repo\rðŸ‘‰ Github Repository\n","date":"2023-12-25T00:00:00Z","image":"https://imadharir.github.io/p/product-management-application/list_products_hu4627790144022541967.png","permalink":"https://imadharir.github.io/p/product-management-application/","title":"Product Management Application"}]