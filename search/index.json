[{"content":"Amazon Book Scraper and Storage\rProject Overview\rThe Amazon Book Scraper and Storage project is an automated data pipeline that scrapes book information from Amazon and stores it in a PostgreSQL database. The project leverages Apache Airflow for orchestration and scheduling of the scraping and data ingestion process.\nTable of Contents\rProject Overview\rArchitecture\rFeatures\rTechnologies Used\rSetup and Installation\rUsage\rArchitecture\rThe project follows a simple ETL (Extract, Transform, Load) architecture:\rExtract: Data is scraped from Amazon using a Python script with requests and BeautifulSoup. Transform: The extracted data is cleaned and structured using pandas. Load: The structured data is inserted into a PostgreSQL database using airflow.providers.postgres.hooks.postgres. High-Level Architecture Components:\rAirflow DAG: Manages and schedules the daily data pipeline tasks. Python Scraper: Extracts book data (title, author, price, rating) from Amazon. PostgreSQL Database: Stores the scraped book data. Google Cloud Composer: Manages the Airflow orchestration in a cloud environment. Features\rAutomated Daily Scraping: The pipeline runs daily, automatically scraping new book data. Data Deduplication: Ensures only new books are added during each subsequent run. Cloud-Based Orchestration: Uses Google Cloud Composer for cloud-based execution and management. Modular Design: Components are decoupled, making it easy to extend or modify parts of the pipeline. Technologies Used\rApache Airflow: Orchestrates and schedules the tasks. Python: Implements the scraping logic using requests and BeautifulSoup. PostgreSQL: Stores the book data for further analysis. Google Cloud Composer: Manages the Airflow environment on Google Cloud. Terraform: Automates the infrastructure setup. Setup and Installation\rNote: This setup guide is intended for Windows users. If you\u0026rsquo;re using a different operating system, please refer to the official documentation for installation instructions specific to your platform.\nPrerequisites\rBefore starting the project setup, ensure you have the following tools and accounts configured:\nGoogle Cloud Platform (GCP) Account\nA GCP account is required to create and manage cloud resources. If you don\u0026rsquo;t have an account, create one at cloud.google.com. Google Cloud Project\nCreate a new GCP project or use an existing one. Make a note of the project ID, as it will be used throughout the setup. Install Terraform\nDownload and install Terraform from terraform.io. Verify the installation by running: 1 terraform -v Install Google Cloud SDK (gcloud)\nInstall the Google Cloud SDK by following the instructions here. Initialize gcloud and set the default project: 1 2 gcloud init gcloud config set project \u0026lt;YOUR_PROJECT_ID\u0026gt; Enable Required APIs\nEnsure the following APIs are enabled in your GCP project: Compute Engine API Cloud Composer API Cloud SQL Admin API Identity and Access Management (IAM) API Cloud Storage API Enable them using the gcloud command: 1 2 3 4 5 gcloud services enable compute.googleapis.com \\ composer.googleapis.com \\ sqladmin.googleapis.com \\ iam.googleapis.com \\ storage.googleapis.com Step 1: Set Up Service Account\rCreate a Service Account\nGo to the IAM \u0026amp; Admin Console and create a new service account with the following details: Name: terraform-admin Role: Owner, roles/roleAdmin, roles/iam.securityAdmin Generate a JSON Key\nAfter creating the service account, generate a JSON key and download it. Store the JSON key in a secure location, as it will be used by Terraform to authenticate with GCP. Set Environment Variable for Authentication\nSet the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your service account key file: 1 export GOOGLE_APPLICATION_CREDENTIALS=\u0026#34;/path/to/service-account-key.json\u0026#34; Step 2: Clone the Repository and Configure Terraform\rClone the Project Repository\nClone the repository containing the Terraform configurations and DAGs: 1 2 git clone https://github.com/imadharir/AmazonBookScaperETL.git cd AmazonBookScaperETL Update terraform.tfvars\nCreate or edit a terraform.tfvars file in the root directory with your project-specific values: 1 2 3 project_id = \u0026#34;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026#34; region = \u0026#34;us-central1\u0026#34; credentials_file = \u0026#34;/path/to/service-account-key.json\u0026#34; Initialize and Validate Terraform Configuration\nInitialize the Terraform workspace: 1 terraform init Validate the configuration to ensure everything is set up correctly: 1 terraform validate Step 3: Deploy the Infrastructure\rDeploy the Resources\nDeploy the infrastructure using Terraform: 1 terraform apply Review the plan and confirm by typing yes. Verify the Deployment\nCheck the GCP console to ensure that all the resources (e.g., Cloud Composer environment, Cloud SQL instance, VPC, etc.) have been created successfully. Step 4: Cloud Composer Configuration\rConfigure Airflow Connections\nIf you didn\u0026rsquo;t create the Airflow connections using Terraform, go to the Cloud Composer UI and configure the necessary connections: Postgres Connection: Conn Id: books_connection Conn Type: Postgres Host: \u0026lt;Your Cloud SQL Internal IP\u0026gt; Schema: amazon_books Login: postgres Password: \u0026lt;Your Password\u0026gt; Port: 5432 Upload DAG Files\nUpload the DAG file(s) to the dags folder in the Cloud Composer bucket: 1 gsutil cp /local/path/to/dag.py gs://\u0026lt;COMPOSER_BUCKET_NAME\u0026gt;/dags/ Step 5: Monitoring and Logs\rMonitor the DAG Runs\nNavigate to the Airflow UI and monitor your DAG runs. Step 6: Clean Up Resources\rTo avoid incurring charges for unused resources, remember to delete the GCP project or individual resources when the project is no longer needed:\n1 terraform destroy Usage\rThe project runs on a daily basis, ensuring the database is updated with new book entries. Can be used for trend analysis, pricing insights, or recommendation systems. ","date":"2024-09-30T00:00:00Z","image":"https://imadharir.github.io/p/amazon-books-scraper/AmazonBookScraper_hu3766364554707935124.png","permalink":"https://imadharir.github.io/p/amazon-books-scraper/","title":"Amazon Books Scraper"},{"content":"Google Cloud Pub/Sub Infrastructure with Terraform\rThis project automates the deployment of Google Cloud Pub/Sub resources using Terraform. The infrastructure includes a Pub/Sub topic and subscription, allowing for messaging between different components of the system.\nProject Architecture\rBelow is the architecture of the project:\nResources Deployed\rPub/Sub Topic: A topic for sending messages. Pub/Sub Subscription: A subscription to recpip install dotenveive messages from the topic. Cloud Storage Bucket: A general-purpose bucket for data storage and ingestion. Cloud Function Storage Bucket: A bucket to store the zip file containing the Cloud Function code. Cloud Function: A serverless function triggered by Cloud Storage events to process data and load it into BigQuery. BigQuery Dataset: A dataset for organizing tables in BigQuery. BigQuery Table: A table for storing stock market data, with schema auto-detected. Key Components\rmain.tf: The entry point for the Terraform configuration, defining the provider (Google Cloud) and project-level settings. pubsub.tf: Contains the configuration for creating the Pub/Sub topic and subscription. function.tf: Defines the setup for the Google Cloud Function that processes data from Cloud Storage. functionBucket.tf: Manages the Cloud Storage bucket specifically for storing Cloud Function code. dataset.tf: Creates the BigQuery dataset that holds the table for stock market data. bq_table.tf: Defines the BigQuery table for storing data, with schema auto-detection enabled. bucket.tf: Defines another bucket for Cloud Storage, used for data ingestion purposes. variables.tf: Defines variables such as project ID, region, topic name, and subscription name to make the configuration flexible. terraform.tfvars: This file contains the values for the variables defined in variables.tf, making it easy to customize for different environments. Cloud Function Overview\rThe Cloud Function is triggered when a JSON file is uploaded to the Cloud Storage bucket. It loads the data into BigQuery, automatically detecting the schema of the JSON file.\nPrerequisites\rTerraform v1.0.0 or later A Google Cloud account A Google Cloud project with billing enabled Google Cloud SDK installed on your local machine A service account with appropriate permissions (Pub/Sub, Cloud Functions, BigQuery, Storage) Zip file containing the Cloud Function code (main.py and requirements.txt) How to Use\rClone the repository:\n1 2 git clone https://github.com/imadharir/gcp-data-pipeline-stock-market.git cd gcp-data-pipeline-stock-market Install Terraform\nSet up a virtual environment:\n1 2 python3 -m venv venv venv\\Scripts\\activate Install dependencies::\n1 pip install -r requirements.txt Grant permissions to the Google Cloud Function service account: Ensure that the service account used by Google Cloud Function has the necessary permissions to access Pub/Sub, BigQuery, and Cloud Storage. You can do this by adding the following roles to the service account: Pub/Sub Publisher, Pub/Sub Subscriber, BigQuery Data Editor, Storage Object Admin. The default service account is typically:\n1 [PROJECT_ID]@appspot.gserviceaccount.com Prepare the environment:\n1 export GOOGLE_APPLICATION_CREDENTIALS=\u0026#34;path/to/your-service-account-key.json\u0026#34; Initialize Terraform:\n1 terraform init Plan the infrasctructure:\n1 terraform plan Review the changes that Terraform will make.\nApply the infrastructure:\n1 terraform apply Confirm the application by typing \u0026lsquo;yes\u0026rsquo; when prompted.\nUpload the Cloud Function zip file: Ensure the zip file containing the Cloud Function code (main.py, requirements.txt) is in the correct path and gets uploaded to the specified bucket.\nCleanup\rTo delete the resources created by Terraform, you can run:\n1 terraform destroy ","date":"2024-09-30T00:00:00Z","image":"https://imadharir.github.io/p/stock-market-real-time-data-analysis/StockMarketDiagram_hu10633608879198561801.jpg","permalink":"https://imadharir.github.io/p/stock-market-real-time-data-analysis/","title":"Stock Market Real Time Data Analysis"}]